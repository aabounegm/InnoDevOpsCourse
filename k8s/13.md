# StatefulSets

## Commands outputs

- Deployment of the StatefulSet:

```powershell
PS C:\Users\aabou> kubectl get po,sts,svc,pvc
NAME               READY   STATUS    RESTARTS   AGE
pod/app-python-0   1/1     Running   0          43s
pod/app-python-1   1/1     Running   0          32s
pod/app-python-2   1/1     Running   0          27s

NAME                          READY   AGE
statefulset.apps/app-python   3/3     43s

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/app-python   LoadBalancer   10.98.131.206   <pending>     5000:31004/TCP   43s
service/kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          13d

NAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/visits-app-python-0   Bound    pvc-3118250a-f2a7-46ba-801e-9be5165c0d1a   256M       RWO            standard       43s
persistentvolumeclaim/visits-app-python-1   Bound    pvc-60d05024-1fc0-4a88-8b32-c6e2ca041d88   256M       RWO            standard       32s
persistentvolumeclaim/visits-app-python-2   Bound    pvc-0dd7e8f6-90b9-4a2a-b26c-397dd9f110b8   256M       RWO            standard       27s
```

- Content of the file in each pod:

```powershell
PS C:\Users\aabou> kubectl exec pods/app-python-0 -- cat /home/app/data/visits.json
344
PS C:\Users\aabou> kubectl exec pods/app-python-1 -- cat /home/app/data/visits.json
339
PS C:\Users\aabou> kubectl exec pods/app-python-2 -- cat /home/app/data/visits.json
334
```

## Explanation

### Difference between output for the replicas

The replicas do not display the same output since they have separate Persistent Volumes assigned to each of them.
That is because the `volumeClaimTemplates` is used, which defines a template for the Volume Claim to be created for each replica.
Consequently, each pod (replica) is operating over a different file.

### Unnecessity of app ordering guarantee

For the logic of our application, the replicas do not need to share a common state.
Each instance is independent of the others and thus ordering does not give us any benefits, and it only slows down the startup.
This can be avoided by setting `podManagementPolicy` to `"Parallel"` in the StatefulSet spec.

## Update Strategies

Kubernetes supports different strategies for rolling out new updates:

- **Recreate**: Terminate the old version and then release the new one. This works best for a development environment, but it ensues a lot of downtime.
- **Ramped**: A slower strategy that allows for a gradual rollout of new versions. Similar to the recreate strategy, but it only replaces one replica after the other instead of the entire system. This can lead to different versions of the same app being deployed at the same time with no control over which one receives the traffic, and it can take some time to complete.
- **Blue/Green**: The new version is deployed alongside the old one, but no traffic is directed to it yet. Only after it is tested and ready will the traffic be switched to it and the old replicas will be deleted. This fixes the downtime and versioning issues, but requires double the resources.
- **Canary**: A combination of the blue/green and ramped strategies. It deploys two versions of the app, one of which is kept up to date and the other is not. The traffic is split between the two versions in proportion to the number of replicas. This allows testing the new version with a small subset of the users, and then switching everyone to the new version once it is ready.
- **A/B Testing**: Rather than splitting traffic by deployed resources as with the canary strategy, this strategy targets a given subset of users depending on some parameters such as the device they are using. It can be implemented using the canary strategy.


_Reference: https://blog.container-solutions.com/kubernetes-deployment-strategies_
